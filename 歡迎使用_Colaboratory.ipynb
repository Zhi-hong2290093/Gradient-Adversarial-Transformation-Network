{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "歡迎使用 Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhi-hong2290093/Gradient-Adversarial-Transformation-Network/blob/master/%E6%AD%A1%E8%BF%8E%E4%BD%BF%E7%94%A8_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1ksQUGZ6ZV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GATN_FC(nn.Module):\n",
        "    '''\n",
        "    USAGE: atn = GATN_FC()\n",
        "\n",
        "    This is FC GATN that takes also gradient as input and inner layer with low dimension\n",
        "\n",
        "    CONTRIBUTER: henryliu, 07.25\n",
        "    '''\n",
        "\n",
        "    def __init__(self, beta=1.5, innerlayer=200, width=28, channel=1):\n",
        "        '''\n",
        "            The sturcture of this network is:\n",
        "            INPUT: x_grad, x_image\n",
        "            OUTPUT: x_adv\n",
        "            STRUCTURE:\n",
        "            (x_image, x_grad) -> layer1{\n",
        "                Linear,\n",
        "                SELU,\n",
        "            }\n",
        "            layer1 -> layer2{\n",
        "                Linear,\n",
        "                SELU,\n",
        "            }\n",
        "            (layer2 - x_grad) ->layer3{\n",
        "                Linear\n",
        "                SELU\n",
        "            }\n",
        "            layer3 -> norm_layer{\n",
        "                sigmoid\n",
        "            } \n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.channel = channel\n",
        "        self.width = width\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(2*width * width * channel, innerlayer),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(innerlayer,width * width * channel),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(width * width * channel, width * width * channel),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        # Xavier initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.torch.nn.init.xavier_normal_(m.weight)\n",
        "        self.out_image = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x_image, x_grad):\n",
        "        self.batch_size = x_image.size(0)\n",
        "        x_image = x_image.view(x_image.size(0), -1)\n",
        "        x_grad = x_grad.view(x_grad.size(0), -1)\n",
        "        x = torch.cat((x_image, x_grad), dim=1)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x + 0.4 * x_grad)\n",
        "        x = self.out_image( (x + x_image-0.5)*5 ) # adding pertubation and norm\n",
        "        return x.view(x.size(0), self.channel, self.width, self.width)\n",
        "\n",
        "class GATN_Conv(nn.Module):\n",
        "    '''\n",
        "    USAGE: atn = GATN_Conv()\n",
        "\n",
        "    This is ATN_a that takes also gradient as input and use convolutional layer\n",
        "\n",
        "    CONTRIBUTER: henryliu, 07.25\n",
        "    '''\n",
        "\n",
        "    def __init__(self, beta=1.5, innerlayer=100, width=28, channel=1):\n",
        "        '''\n",
        "        This is a simple net of (width * width)->FC( innerlayer )->FC(width * width)-> sigmoid(grad + output)\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.channel = channel\n",
        "        self.width = width\n",
        "        self.layer1_conv = nn.Sequential(  # input shape (1, 28, 28)\n",
        "            nn.Conv2d(1, 8, 5, 1), # use padding = 0\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # output shape (10, 14, 14)\n",
        "        )\n",
        "        self.layer2_conv = nn.Sequential(  # input shape (1, 28, 28)\n",
        "            nn.Conv2d(1, 8, 5, 1), # use padding = 0\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # output shape (10, 14, 14)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear( 8 * (width - 4)/2 * (width - 4)/2 * 2, width * width * channel),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Linear(width * width * channel, width * width * channel)\n",
        "        )\n",
        "        # Xavier initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "                nn.init.torch.nn.init.xavier_normal_(m.weight)\n",
        "        self.out_image = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x_image, x_grad):\n",
        "        self.batch_size = x_image.size(0)\n",
        "        x1 = self.layer1_conv(x_image).view(self.batch_size, -1)\n",
        "        x2 = self.layer2_conv(x_grad).view(self.batch_size, -1)\n",
        "        x = torch.cat((x1, x2), dim=1) # [D, 10 * width/4 *width /4 * 2]\n",
        "        x_image = x_image.view(x_image.size(0), -1)\n",
        "        x_grad = x_grad.view(x_grad.size(0), -1)        \n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x + x_grad) # x is perturbation\n",
        "        x = self.out_image( (x + x_image-0.5)*5 ) # adding pertubation and norm\n",
        "        return x.view(x.size(0), self.channel, self.width, self.width)\n",
        "\n",
        "class ATN_a(nn.Module):\n",
        "    '''\n",
        "   DO NOT USE THIS\n",
        "   THIS IS FAILED TRY\n",
        "    '''\n",
        "    def __init__(self, alpha=3, beta=1.5, innerlayer=2000, width=28, channel=1):\n",
        "        '''\n",
        "            This is a simple net of (width * width)->FC( innerlayer )->FC(width * width)\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.channel = channel\n",
        "        self.width = width\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(width * width * channel, innerlayer),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(innerlayer, innerlayer),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.layer3 = nn.Linear(innerlayer, width * width * channel)\n",
        "        self.outlayer = nn.Sigmoid()\n",
        "        self.weight_a = torch.tensor([1.5], requires_grad=False)\n",
        "        self.weight_b = torch.tensor([0.5], requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = (x - self.weight_b) * self.weight_a\n",
        "        x = self.outlayer(x)\n",
        "        return x.view(x.size(0), self.channel, self.width, self.width)\n",
        "\n",
        "\n",
        "class ATN_b(nn.Module):\n",
        "    '''\n",
        "    DO NOT USE THIS\n",
        "    THIS IS FAILED TRY\n",
        "    '''\n",
        "    def __init__(self, alpha=3, beta=1.5, innerlayer=1000, width=28, channel=1):\n",
        "        '''\n",
        "            The network is following\n",
        "            layer1: {\n",
        "                conv,\n",
        "                ReLU,\n",
        "                Maxpooling\n",
        "            }\n",
        "            layer2:{\n",
        "                conv,\n",
        "                ReLU,\n",
        "                Maxpooling\n",
        "            }\n",
        "            layer:3 {\n",
        "                FC,\n",
        "                Sigmoid\n",
        "            }\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.channel = channel\n",
        "        self.width = width\n",
        "        self.layer1_conv = nn.Sequential(  # input shape (1, 28, 28)\n",
        "            nn.Conv2d(1, 8, 3, 1, 1),  \n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2),  # output shape (8, 14, 14)\n",
        "        )\n",
        "        self.layer2_conv = nn.Sequential(  # input shape (8, 14, 14)\n",
        "            nn.Conv2d(8, 16, 3, 1, 1),  \n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2),  # output shape (16, 7, 7 )\n",
        "        )\n",
        "        self.layer3_fc = nn.Sequential(\n",
        "            nn.Linear(width/4 * width/4 * channel * 16, channel * width * width),\n",
        "        )\n",
        "        self.outlayer = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1_conv(x)\n",
        "        x = self.layer2_conv(x)\n",
        "        x = x.view(x.size(0), self.width/4 * self.width/4 * self.channel * 16 )\n",
        "        x = self.layer3_fc(x)\n",
        "        x = x - 0.5\n",
        "        x = x.outlayer(x)\n",
        "        return x.view(x.size(0), self.channel, self.width, self.width)\n",
        "\n",
        "\n",
        "class GATN_a(nn.Module):\n",
        "    ''' \n",
        "    DO NOT USE THIS\n",
        "    THIS IS FAILED TRY\n",
        "    '''\n",
        "\n",
        "    def __init__(self, alpha=3, beta=1.5, innerlayer=2000, width=28, channel=1):\n",
        "        '''\n",
        "            This is a simple net of (width * width)->FC( innerlayer )->FC(width * width)-> sigmoid(grad + output)\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.channel = channel\n",
        "        self.width = width\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(2*width * width * channel, innerlayer),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(innerlayer, innerlayer),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(innerlayer, width * width * channel),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.grad_linear = nn.Linear(width * width * channel, width*width*channel)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.torch.nn.init.xavier_normal_(m.weight)\n",
        "        self.weight_v = torch.ones(channel * width * width, requires_grad=True)  # MSL_ave of x_grad is around 0.03 / abs is 0.1 \n",
        "        self.weight_res = torch.tensor([0.02], requires_grad = False ) #0.6\n",
        "        self.out_image = nn.Sigmoid()\n",
        "    def forward(self, x_image, x_grad):\n",
        "        self.batch_size = x_image.size(0)\n",
        "\n",
        "        x_image = x_image.view(x_image.size(0), -1)\n",
        "        x_grad = x_grad.view(x_grad.size(0), -1)\n",
        "        x = torch.cat((x_image, x_grad), dim=1)\n",
        "        x = self.layer1(x)\n",
        "        #x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        # should this grad be + or -?\n",
        "        # MSE_ave of x is around 4.7 / abs is 0.7\n",
        "        #x= self.out_perturb( ) #perturbation\n",
        "        x = self.out_image( (x * self.weight_res + self.grad_linear(x_grad) +x_image-0.5)*5 ) # adding pertubation and norm\n",
        "        return x.view(x.size(0), self.channel, self.width, self.width)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}